{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This example accepts transcript data (in Microsoft Word, docx) and transform them to the JSON format our tool needs. The format:\n",
    "\n",
    "```\n",
    "Speaker1: \n",
    "This is the first line.\n",
    "\n",
    "Speaker2: \n",
    "This is the second line.\n",
    "```\n",
    "\n",
    "In addition, it will extract comments from the docx and treat them as human open coding results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in c:\\users\\john chen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\john chen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (6.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\john chen\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-docx) (4.13.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python-docx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Converter\n",
    "The converter code here is generated by Gemini-2.5-pro with slight manual editing. Below is the prompt:\n",
    "```\n",
    "Write Python code to convert all `.docx` files in an input folder into the format of the uploaded example JSON. The format of the docx:\n",
    "\n",
    "===\n",
    "{$Name (May have whitespaces)} {$Time}\n",
    "{$Content}\n",
    "===\n",
    "\n",
    "Following a name and time denotation, each non-empty paragraph should be its own \"item\". For each, identify its \"paragraph\" number for further processing.\n",
    "Ignore content before the first name. If no time information is available, mark it as \"1970-01-01 00:00:00\".\n",
    "The entirety should be considered as one chunk. The chunk ID should come from the file name. Return the JS object.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_single_docx(docx_filepath):\n",
    "    \"\"\"\n",
    "    Converts a single .docx file into a structured dictionary based on speaker text.\n",
    "    Each item is tagged with its paragraph index for reliable comment mapping.\n",
    "\n",
    "    Args:\n",
    "        docx_filepath (str): The full path to the .docx file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the structured data from the document,\n",
    "              or None if the file cannot be processed.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(docx_filepath):\n",
    "        print(f\"Error: File not found at {docx_filepath}\")\n",
    "        return None\n",
    "\n",
    "    chunk_id = os.path.splitext(os.path.basename(docx_filepath))[0]\n",
    "    document = Document(docx_filepath)\n",
    "    \n",
    "    chunk_data = {\n",
    "        \"id\": chunk_id,\n",
    "        \"items\": []\n",
    "    }\n",
    "\n",
    "    current_uid = None\n",
    "    current_time = \"1970-01-01 00:00:00\"\n",
    "    item_counter = 1\n",
    "    first_speaker_found = False\n",
    "    \n",
    "    # Regex to capture 'Name HH:MM:SS:' or 'Name YYYY-MM-DD HH:MM:SS:'\n",
    "    name_time_regex = re.compile(r'^(.*?)\\s*((?:\\d{4}-\\d{2}-\\d{2}\\s)?\\d{2}:\\d{2}:\\d{2})$')\n",
    "\n",
    "    # Use enumerate to get the paragraph index (para_counter)\n",
    "    for para_counter, para in enumerate(document.paragraphs):\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        match = name_time_regex.match(text)\n",
    "        if match:\n",
    "            first_speaker_found = True\n",
    "            current_uid = match.group(1).strip()\n",
    "            time_str = match.group(2).strip()\n",
    "            \n",
    "            if ' ' not in time_str:\n",
    "                time_str = f\"1970-01-01 {time_str}\"\n",
    "            \n",
    "            try:\n",
    "                parsed_time = datetime.strptime(time_str, '%Y-%m-%d %H:%M:%S')\n",
    "                current_time = parsed_time.strftime('%Y-%m-%d %H:%M:%S')\n",
    "            except ValueError:\n",
    "                current_time = \"1970-01-01 00:00:00\"\n",
    "            \n",
    "            continue\n",
    "\n",
    "        if first_speaker_found and current_uid:\n",
    "            content = text\n",
    "            item = {\n",
    "                \"id\": f\"{chunk_id}-{item_counter}\",\n",
    "                \"uid\": current_uid,\n",
    "                \"time\": current_time,\n",
    "                \"content\": content,\n",
    "                \"paragraph_index\": para_counter # Store the paragraph index\n",
    "            }\n",
    "            chunk_data[\"items\"].append(item)\n",
    "            item_counter += 1\n",
    "                \n",
    "    return chunk_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Comment Reader\n",
    "The converter code here is generated by Gemini-2.5-pro with slight manual editing. Below is the prompt:\n",
    "```\n",
    "Write a new procedure to extract comments as open codes from human coders into the format of the uploaded example JSON. Take the docx and the parsed chunk_data as input. Use the paragraph index to identify the comment's position.\n",
    "Separate comments from each coder and send output as a Map<string, CodedData>.\n",
    "Then, edit the folder-level procedure to save 1) the same data file as before, and 2) each coder's codes as a separate JSON.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_comments_as_codes(docx_filepath, chunk_data):\n",
    "    \"\"\"\n",
    "    Extracts comments from a .docx file and formats them as open codes.\n",
    "    Uses the comment's associated paragraph to identify which data item it relates to.\n",
    "    \n",
    "    Args:\n",
    "        docx_filepath (str): The path to the .docx file.\n",
    "        chunk_data (dict): The parsed data from process_single_docx for the same file.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are coder names (comment authors) and\n",
    "              values are the structured coded data.\n",
    "    \"\"\"\n",
    "    coders_data = defaultdict(lambda: {\"threads\": {}})\n",
    "    document = Document(docx_filepath)\n",
    "    chunk_id = chunk_data['id']\n",
    "\n",
    "    # Create a lookup map from paragraph index to item ID\n",
    "    paragraph_index_to_item_id_map = {item['paragraph_index']: item['id'] for item in chunk_data['items']}\n",
    "    \n",
    "    # Build a mapping of comment IDs to paragraph indices by scanning the document XML\n",
    "    comment_to_paragraph_map = {}\n",
    "    \n",
    "    # Scan through paragraphs to find comment range markers\n",
    "    for para_index, para in enumerate(document.paragraphs):\n",
    "        # Check the paragraph's XML element for comment range markers\n",
    "        if para._element is not None:\n",
    "            for element in para._element.iter():\n",
    "                # Look for comment range start markers\n",
    "                if element.tag.endswith('commentRangeStart'):\n",
    "                    comment_id = element.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}id')\n",
    "                    if comment_id:\n",
    "                        comment_to_paragraph_map[comment_id] = para_index\n",
    "    \n",
    "    # Process each comment\n",
    "    for comment in document.comments:\n",
    "        # Get comment properties using correct attribute names\n",
    "        comment_id = str(comment.comment_id) if hasattr(comment, 'comment_id') else None\n",
    "        coder = comment.author if hasattr(comment, 'author') else 'Unknown'\n",
    "        code_text = comment.text.strip() if hasattr(comment, 'text') else ''\n",
    "        \n",
    "        # Skip empty comments or comments without IDs\n",
    "        if not comment_id or not code_text:\n",
    "            continue\n",
    "            \n",
    "        # Find the paragraph this comment is associated with\n",
    "        para_index = comment_to_paragraph_map.get(comment_id)\n",
    "        \n",
    "        if para_index is not None:\n",
    "            # Find the corresponding item ID using paragraph index\n",
    "            item_id = paragraph_index_to_item_id_map.get(para_index)\n",
    "            \n",
    "            if item_id:\n",
    "                # Build the nested structure for the coder's data\n",
    "                thread = coders_data[coder][\"threads\"].setdefault(chunk_id, {\"id\": chunk_id, \"items\": {}})\n",
    "                item_codes = thread[\"items\"].setdefault(item_id, {\"id\": item_id, \"codes\": []})\n",
    "\n",
    "                # Split codes into multiple items using ; , and . as delimiters\n",
    "                codes = re.split(r'[;,.]', code_text)\n",
    "                for code in codes:\n",
    "                    code = code.strip()\n",
    "                    # Add the code if it's not already present\n",
    "                    if code not in item_codes[\"codes\"]:\n",
    "                        item_codes[\"codes\"].append(code)\n",
    "\n",
    "    return dict(coders_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_folder_to_json(input_folder, output_filename='data-example.json'):\n",
    "    \"\"\"\n",
    "    Scans a directory for .docx files, processes them for content and comments,\n",
    "    and saves the output into multiple JSON files.\n",
    "\n",
    "    Args:\n",
    "        input_folder (str): The path to the folder containing .docx files.\n",
    "        output_filename (str, optional): The name for the main data output file.\n",
    "    \"\"\"\n",
    "    final_json_object = {}\n",
    "    aggregated_codes_by_coder = defaultdict(lambda: {\"threads\": {}})\n",
    "    \n",
    "    try:\n",
    "        docx_files = [f for f in os.listdir(input_folder) if f.endswith('.docx')]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The folder '{input_folder}' was not found.\")\n",
    "        return\n",
    "\n",
    "    if not docx_files:\n",
    "        print(f\"No .docx files were found in the folder '{input_folder}'.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(docx_files)} .docx file(s) to process...\")\n",
    "\n",
    "    for docx_filename in docx_files:\n",
    "        full_path = os.path.join(input_folder, docx_filename)\n",
    "        print(f\"Processing '{docx_filename}'...\")\n",
    "        \n",
    "        # 1. Process the document for speaker text content\n",
    "        chunk_data = process_single_docx(full_path)\n",
    "        if chunk_data:\n",
    "            # We need to remove the paragraph_index from the final output for cleanliness\n",
    "            final_chunk_data = chunk_data.copy()\n",
    "            final_chunk_data['items'] = [{k: v for k, v in item.items() if k != 'paragraph_index'} for item in final_chunk_data['items']]\n",
    "            final_json_object[final_chunk_data['id']] = final_chunk_data\n",
    "\n",
    "            # 2. Process the same document for comments (codes)\n",
    "            codes_by_coder = extract_comments_as_codes(full_path, chunk_data)\n",
    "            \n",
    "            # 3. Merge the results into the aggregated coder data\n",
    "            for coder, data in codes_by_coder.items():\n",
    "                for thread_id, thread_data in data[\"threads\"].items():\n",
    "                    # Ensure the thread exists for the coder\n",
    "                    agg_thread = aggregated_codes_by_coder[coder][\"threads\"].setdefault(thread_id, {\"id\": thread_id, \"items\": {}})\n",
    "                    # Merge items from the current doc into the aggregated thread\n",
    "                    agg_thread[\"items\"].update(thread_data[\"items\"])\n",
    "\n",
    "    # --- Save the output files ---\n",
    "\n",
    "    # 1. Save the main data file (same as before)\n",
    "    output_path = os.path.join(os.getcwd(), output_filename)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(final_json_object, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"\\nMain data conversion complete. Output saved to '{output_path}'\")\n",
    "\n",
    "    # 2. Save a separate JSON file for each coder\n",
    "    if not aggregated_codes_by_coder:\n",
    "        print(\"No comments found to generate coder files.\")\n",
    "        return\n",
    "\n",
    "    # Create the folder\n",
    "    os.makedirs(\"human\", exist_ok=True)\n",
    "\n",
    "    print(\"\\nSaving code files for each coder...\")\n",
    "    for coder, coded_data in aggregated_codes_by_coder.items():\n",
    "        coder_filename = f\"human/{coder.replace(' ', '_')}.json\"\n",
    "        coder_output_path = os.path.join(os.getcwd(), coder_filename)\n",
    "        with open(coder_output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(coded_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"  - Saved codes for '{coder}' to '{coder_output_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 .docx file(s) to process...\n",
      "Processing 'example.docx'...\n",
      "\n",
      "Main data conversion complete. Output saved to 'f:\\Minor Solutions\\LLM-Qualitative\\examples\\docx-data\\data-example.json'\n",
      "No comments found to generate coder files.\n",
      "Conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Test the rewritten function\n",
    "root_folder_path = './'\n",
    "output_json_path = convert_folder_to_json(root_folder_path)\n",
    "print(\"Conversion completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
