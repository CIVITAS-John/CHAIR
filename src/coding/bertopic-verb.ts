/**
 * BERTopic-based conversation analysis with verb phrase labels.
 *
 * This analyzer is identical to BertopicAnalyzerAny except for one key difference:
 * it instructs the LLM to generate verb phrases rather than general labels.
 *
 * Verb phrase coding is particularly useful for:
 * - Action-oriented analysis (focusing on what people are doing)
 * - Process-oriented research (examining activities and interactions)
 * - Behavioral studies (describing observable actions)
 * - Grounded theory where verbs capture processes and dynamics
 *
 * The only modification from BertopicAnalyzerAny is the LLM prompt format:
 * - BertopicAnalyzerAny: "Label: {A single label...}"
 * - BertopicAnalyzerVerb: "Phrase: {A single verb phrase...}"
 *
 * This demonstrates how minor prompt engineering changes can dramatically
 * affect the nature of generated codes while keeping the technical pipeline
 * identical.
 *
 * See bertopic-any.ts for detailed documentation of the BERTopic pipeline.
 *
 * @author John Chen
 */

import { writeFileSync } from "fs";
import { dirname, resolve } from "path";
import { fileURLToPath } from "url";

import type { BertopicTopics, CodedThread, Conversation, Message } from "../schema.js";
import { BaseStep } from "../steps/base-step.js";
import { ensureFolder } from "../utils/io/file.js";
import { requestLLM } from "../utils/ai/llms.js";
import { logger } from "../utils/core/logger.js";
import { runPythonScript } from "../utils/runtime/python.js";

import { buildMessagePrompt, ConversationAnalyzer } from "./conversations.js";

/**
 * BERTopic-based analyzer with verb phrase label generation.
 *
 * Identical to BertopicAnalyzerAny except prompts specifically request verb phrases.
 * This results in action-oriented codes like "Asking for clarification" rather than
 * noun-based codes like "Clarification request".
 *
 * @author John Chen
 */
export default class BertopicAnalyzerVerb extends ConversationAnalyzer {
    /** The name of the analyzer. */
    override name = "bertopic-verb";
    /** The base temperature for the LLM. */
    override baseTemperature = 0.5;
    /** The codes generated by Bertopic. */
    #codes: Record<string, string> = {};

    /**
     * Get the chunk size and cursor movement for the LLM.
     * @returns [Chunk size, Cursor movement]
     */
    override getChunkSize(_recommended: number, remaining: number) {
        return remaining;
    }

    /** Preprocess the conversations in batch. */
    override async batchPreprocess(
        conversations: Conversation[],
        _analyzed: CodedThread[],
    ): Promise<void> {
        await logger.withSource(this._prefix, "batchPreprocess", true, async () => {
            const { dataset } = BaseStep.Context.get();
            // Write the messages into the file.
            const messages = conversations.flatMap((conversation) =>
                conversation.items.filter(
                    (message) =>
                        // TODO: Support subchunks
                        "content" in message &&
                        message.content.length > 0 &&
                        (!message.chunk || message.chunk === conversation.id),
                ),
            );
            const content = messages.map((message) =>
                // TODO: Support subchunks
                "content" in message
                    ? buildMessagePrompt(dataset, message, undefined, undefined, true).replaceAll(
                          "\n",
                          " ",
                      )
                    : "",
            );
            ensureFolder("./known");
            writeFileSync("./known/bertopic.temp.json", JSON.stringify(content));
            // Run the Python script
            let topics: BertopicTopics = {};
            const __dirname = dirname(fileURLToPath(import.meta.url));
            await runPythonScript(resolve(__dirname, "bertopic_impl.py"), {
                args: [messages.length.toString()],
                parser: (message) => {
                    if (message.startsWith("{")) {
                        logger.success(message);
                        topics = JSON.parse(message) as BertopicTopics;
                    } else {
                        logger.warn(message);
                    }
                },
            });
            // Generate a label and definition for each topic
            for (const topic of Object.values(topics)) {
                const ids = topic.ids.sort(
                    (A, B) => topic.probabilities[B] - topic.probabilities[A],
                );
                // Maximum 5 examples sorted by probabilities
                const examples = ids.slice(0, 5).map((ID) => messages[ID]);
                // Build the prompt
                const prompt = `
You are an expert in thematic analysis with grounded theory, working on open coding.
You identified a topic from the input quotes. Each quote is independent from another.
${dataset.researchQuestion}
${dataset.codingNotes}${this.customPrompt}

Always follow the output format:
===
Thought: {What is the most common theme among the input quotes? Do not over-interpret the data.}
Phrase: {A single verb phrase that faithfully describes the topic}
===`.trim();
                // Find 5 keywords from the topic
                const keywords = topic.keywords.slice(0, 5);
                // Request the LLM
                const response = await requestLLM(
                    [
                        { role: "system", content: prompt },
                        {
                            role: "user",
                            content: `Quotes:
${examples
    .map((message) =>
        // TODO: Support subchunks
        "content" in message
            ? `- ${buildMessagePrompt(dataset, message, undefined, undefined, true)}`
            : "",
    )
    .join("\n")}
Keywords: ${keywords.join(", ")}`.trim(),
                        },
                    ],
                    `messaging-groups/${this.name}`,
                    this.baseTemperature,
                    false,
                );
                // Parse verb phrase from response
                // NOTE: Only difference from BertopicAnalyzerAny - looks for "Phrase:" not "Label:"
                let phrase = "";
                const lines = response.split("\n");
                for (const _line of lines) {
                    const line = _line.trim();
                    if (line.startsWith("Phrase:")) {
                        phrase = line.slice(7).trim().toLowerCase();
                        if (phrase.endsWith(".")) {
                            phrase = phrase.slice(0, -1);
                        }
                    }
                }
                // Assign messages to the topic
                for (const id of ids) {
                    this.#codes[messages[id].id] = phrase;
                }
            }
        });
    }

    /** Parse the response from the LLM. */
    override parseResponse(
        _analysis: CodedThread,
        _lines: string[],
        subunits: Message[],
        _chunkStart: number,
        _iteration: number,
    ): Promise<Record<number, string>> {
        const results: Record<number, string> = {};
        for (let i = 0; i < subunits.length; i++) {
            let code = this.#codes[subunits[i].id] ?? "";
            // Sometimes, the code ends with a period
            if (code.endsWith(".")) {
                code = code.slice(0, -1);
            }
            // Sometimes, the code is inside a quote
            if (code.startsWith('"') && code.endsWith('"')) {
                code = code.slice(1, -1);
            }
            results[i + 1] = code;
        }
        return Promise.resolve(results);
    }
}
